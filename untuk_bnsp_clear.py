# -*- coding: utf-8 -*-
"""UNTUK_BNSP_CLEAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y6hXyGAmdh6iqgeP3GKPensy1r-2vdTP
"""

import pandas as pd

df = pd.read_csv('/content/data mc.csv', encoding='latin1')
display(df.head())

df.info()

df = df.drop(['No.', 'Nama File', 'Item', 'Part No', 'Cust', 'Date'], axis=1)
display(df.head())

cols_to_convert = [' Insert', ' Oil Cost', ' Jig, Tool & Maint', ' Man Power Direct', ' Electricity', ' Depresiasi Mesin', ' TOTAL COST MC', 'Jumlah mesin', 'Casting Weight', 'Man Power', 'MCT TOTAL', 'PRODUKTIFITAS LINE', 'TOTAL PROFIT AMOUNT', 'Need Line']

# Remove leading spaces from column names in the DataFrame
df.columns = df.columns.str.strip()

# Remove leading spaces from column names in the list
cols_to_convert = [col.strip() for col in cols_to_convert]

for col in cols_to_convert:
  # Replace non-numeric strings with NaN and then convert to numeric
  df[col] = df[col].replace(['N/A', '#VALUE!', 'Not found'], pd.NA)
  df[col] = pd.to_numeric(df[col], errors='coerce')

display(df.info())

display(df.sample(20))

cols_to_convert_to_int = ['Insert', 'Oil Cost', 'Jig, Tool & Maint', 'Man Power Direct', 'Electricity', 'Depresiasi Mesin', 'TOTAL COST MC']

for col in cols_to_convert_to_int:
  # Convert to nullable integer type
  df[col] = df[col].astype('Int64')

display(df.info())

display(df.sample(20))

cols_to_check_for_na = ['Insert', 'Oil Cost', 'Jig, Tool & Maint', 'Man Power Direct', 'Electricity', 'Depresiasi Mesin', 'TOTAL COST MC']
df.dropna(subset=cols_to_check_for_na, inplace=True)
display(df.info())

display(df.isnull().sum())

rows_with_na = df[df['TOTAL PROFIT AMOUNT'].isnull()]
display(rows_with_na)

display(df.duplicated().sum())

duplicate_rows = df[df.duplicated()]
display(duplicate_rows)

df.drop_duplicates(inplace=True)
display(df.head())
display(df.duplicated().sum())

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
corr_matrix = df.corr()

# Create a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numeric Columns')
plt.show()

display(df.sample(20))

from statsmodels.stats.outliers_influence import variance_inflation_factor
import numpy as np # Import numpy for handling inf values later if needed

# Drop columns with NaN values in 'TOTAL PROFIT AMOUNT' before calculating VIF
# VIF calculation requires no missing values in the input data
df_cleaned = df.dropna(subset=['TOTAL PROFIT AMOUNT']).copy()

# Select only numeric columns for VIF calculation
numeric_cols = df_cleaned.select_dtypes(include=['float64', 'Int64']).columns

# Convert nullable integers to float64 for VIF calculation compatibility
for col in numeric_cols:
  if df_cleaned[col].dtype == 'Int64':
    df_cleaned[col] = df_cleaned[col].astype('float64')


# Drop the target variable if it's in the numeric columns after type conversion
cols_to_exclude = ['TOTAL COST MC']
numeric_cols = numeric_cols.drop(cols_to_exclude, errors='ignore')


X = df_cleaned[numeric_cols]

# Calculate VIF for each predictor variable
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns

# Handling potential infinite VIF values when a feature can be perfectly predicted by others
vif_values = []
for i in range(X.shape[1]):
    try:
        vif = variance_inflation_factor(X.values, i)
        vif_values.append(vif)
    except np.linalg.LinAlgError: # Catch potential singular matrix errors
        vif_values.append(np.inf) # Assign infinity if VIF cannot be calculated

vif_data["VIF"] = vif_values # Assign vif_values to the 'VIF' column here

display(vif_data.sort_values(by='VIF', ascending=False))

# Assuming corr_matrix was calculated in a previous step
# If not, recalculate it here:
# corr_matrix = df.corr()

display(corr_matrix['TOTAL COST MC'].sort_values(ascending=False))

cols_to_drop = ['Insert', 'Oil Cost', 'Jig, Tool & Maint', 'Man Power Direct', 'Electricity', 'Jumlah mesin', 'Man Power', 'PRODUKTIFITAS LINE', 'TOTAL PROFIT AMOUNT', 'Need Line']
df = df.drop(cols_to_drop, axis=1)
display(df.head())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define features (X) and target (y) based on the specified columns
features = ['Depresiasi Mesin', 'Casting Weight', 'MCT TOTAL']
target = 'TOTAL COST MC'

X = df[features]
y = df[target]

# Split data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# Initialize and train Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)
y_pred_gb = gb_model.predict(X_test)

# Evaluate models
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

mae_gb = mean_absolute_error(y_test, y_pred_gb)
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

# Display results
print("Random Forest Regressor Evaluation:")
print(f"  Mean Absolute Error (MAE): {mae_rf:.2f}")
print(f"  Mean Squared Error (MSE): {mse_gb:.2f}")
print(f"  R-squared (R2): {r2_rf:.2f}")
print("\nGradient Boosting Regressor Evaluation:")
print(f"  Mean Absolute Error (MAE): {mae_gb:.2f}")
print(f"  Mean Squared Error (MSE): {mse_gb:.2f}")
print(f"  R-squared (R2): {r2_gb:.2f}")

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Gradient Boosting Regressor
param_grid = {
    'n_estimators': [100, 200, 300],  # Number of boosting stages
    'learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage
    'max_depth': [3, 4, 5], # Maximum depth of the individual regression estimators
    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4] # Minimum number of samples required to be at a leaf node
}

# Initialize GridSearchCV
# Using the Gradient Boosting model that performed better
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Perform the grid search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_mse = -grid_search.best_score_ # Convert negative MSE to positive

print("Best Hyperparameters:", best_params)
print(f"Best Cross-validated MSE: {best_mse:.2f}")

# Evaluate the best model on the test set
best_gb_model = grid_search.best_estimator_
y_pred_tuned_gb = best_gb_model.predict(X_test)

mae_tuned_gb = mean_absolute_error(y_test, y_pred_tuned_gb)
mse_tuned_gb = mean_squared_error(y_test, y_pred_tuned_gb)
r2_tuned_gb = r2_score(y_test, y_pred_tuned_gb)

print("\nTuned Gradient Boosting Regressor Evaluation on Test Set:")
print(f"  Mean Absolute Error (MAE): {mae_tuned_gb:.2f}")
print(f"  Mean Squared Error (MSE): {mse_tuned_gb:.2f}")
print(f"  R-squared (R2): {r2_tuned_gb:.2f}")

import pickle

# Define the filename for the pickle file
model_filename = 'tuned_gradient_boosting_model.pkl'

# Save the trained model to a pickle file
with open(model_filename, 'wb') as file:
    pickle.dump(best_gb_model, file)

print(f"The tuned Gradient Boosting model has been saved to '{model_filename}'")

import streamlit as st
import pickle
import pandas as pd

# Load the trained model
try:
    with open('tuned_gradient_boosting_model.pkl', 'rb') as file:
        model = pickle.load(file)
except FileNotFoundError:
    st.error("Model file not found. Please make sure 'tuned_gradient_boosting_model.pkl' is in the same directory.")
    st.stop() # Stop the app if the model file is not found
except Exception as e:
    st.error(f"Error loading the model: {e}")
    st.stop()


st.title('TOTAL COST MC Prediction App')

st.write('Enter the values for the features to predict the TOTAL COST MC.')

# Create input fields for the features
depresiasi_mesin = st.number_input('Depresiasi Mesin', min_value=0.0, value=1000.0)
casting_weight = st.number_input('Casting Weight', min_value=0.0, value=1.0)
mct_total = st.number_input('MCT TOTAL', min_value=0.0, value=1.0)

# Create a button to make predictions
if st.button('Predict TOTAL COST MC'):
    # Create a DataFrame with the input values
    input_data = pd.DataFrame({
        'Depresiasi Mesin': [depresiasi_mesin],
        'Casting Weight': [casting_weight],
        'MCT TOTAL': [mct_total]
    })

    # Make prediction
    try:
        prediction = model.predict(input_data)
        st.success(f'Predicted TOTAL COST MC: {prediction[0]:,.2f}')
    except Exception as e:
        st.error(f"Error during prediction: {e}")

st.write("Note: This is a simplified example for demonstration purposes.")